import streamlit as st
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
import ollama
import os

# Initialize models and vector store
EMBEDDING_MODEL = "toshk0/nomic-embed-text-v2-moe:Q6_K"
LLM_MODEL = "gemma3:4b"
DB_DIR = "db/db_nomic_embed_v2_noq_moe_chroma"
TEXT_SOURCE = "bengali_kb/doc1_noq.txt"

CHUNK_SIZE = 600
CHUNK_OVERLAP = 150

# Set up Streamlit page configuration
st.set_page_config(page_title="Bengali RAG Chatbot", layout="wide")
st.title("üí¨ Bengali RAG Chatbot")

st.markdown(
    f"ü§ñü§ñ LLM Model: <i><b>{LLM_MODEL}</b></i>, üì¶‚úàÔ∏è Embedding Model: <i><b>{EMBEDDING_MODEL}</b></i>",
    unsafe_allow_html=True
)

# Custom CSS for dynamic background colors
st.markdown("""
    <style>
    /* Style for assistant's response while streaming (typing) */
    .streaming {
        background-color: #FFFACD; /* Light yellow for typing */
        padding: 10px;
        border-radius: 5px;
    }
    /* Style for assistant's response when complete */
    .complete {
        background-color: #E0FFE0; /* Light green for complete */
        padding: 10px;
        border-radius: 5px;
    }
    /* Style for thinking indicator */
    .think {
        font-style: italic;
    }
    </style>
""", unsafe_allow_html=True)

# Initialize vector store in session state to avoid reloading
if "vectorstore" not in st.session_state:
    loader = TextLoader(TEXT_SOURCE, encoding="utf-8")
    docs = loader.load()
    st.write("üìïüìï Text document loaded")

    # Split the document into chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunks = splitter.split_documents(docs)

    # Load embeddings and create/load vector store
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

    if os.path.exists(f"{DB_DIR}/chroma.sqlite3"):
        st.write("üîëüì¶ Embedding exists! Loading it...")
        st.session_state.vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
    else:
        st.write("üì¶‚ùå Embedding does not exist, creating...")
        st.session_state.vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=DB_DIR)
        st.write("üîíüîí Vector store creation complete")

# Initialize chat history in session state
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "‡¶π‡ßç‡¶Ø‡¶æ‡¶≤‡ßã! ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ RAG ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡¶¨‡¶ü‡•§ ‡¶°‡¶ï‡ßÅ‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶∏‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§"}
    ]

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message["role"] == "assistant":
            st.markdown(f'<div class="complete">{message["content"]}</div>', unsafe_allow_html=True)
        else:
            st.markdown(message["content"])

# Function to retrieve context
def retrieve_context(query: str) -> str:
    results = st.session_state.vectorstore.similarity_search(query, k=5)
    return "\n\n".join([doc.page_content for doc in results])

# Function to generate streaming answer
def generate_answer(query: str, context: str):
    stream = ollama.chat(
        model=LLM_MODEL,
        messages=[
            {"role": "user", "content": f"‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó‡¶ü‡¶ø ‡¶™‡¶°‡¶º‡ßã ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶ì ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì‡•§\n\n‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó:\n{context}"},
            {"role": "user", "content": query}
        ],
        stream=True
    )
    return stream

# Input field for user messages
if user_input := st.chat_input("Type your message here..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # Generate streaming response
    try:
        context = retrieve_context(user_input)
        
        # Create a placeholder for the streaming response
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            # Stream the response
            stream = generate_answer(user_input, context)
            for chunk in stream:
                if 'message' in chunk and 'content' in chunk['message']:
                    full_response += chunk['message']['content']
                    response_placeholder.markdown(
                        f'<div class="streaming">{full_response}‚ñå</div>',
                        unsafe_allow_html=True
                    )
            
            # Finalize the response with complete style
            response_placeholder.markdown(
                f'<div class="complete">{full_response}</div>',
                unsafe_allow_html=True
            )
        
        # Add the complete assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": full_response})
            
    except Exception as e:
        st.error(f"Error communicating with {LLM_MODEL}: {str(e)}")
        st.session_state.messages.append({"role": "assistant", "content": "Sorry, I encountered an error. Please try again!"})
        with st.chat_message("assistant"):
            st.markdown(
                f'<div class="complete">Sorry, I encountered an error. Please try again!</div>',
                unsafe_allow_html=True
            )