from langchain_community.document_loaders import TextLoader,PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from ollama import chat
import os, time, threading, winsound

#jeffh/intfloat-multilingual-e5-large-instruct:f16
#snowflake-arctic-embed2
#bge-m3
#granite-embedding:278m
#toshk0/nomic-embed-text-v2-moe:Q6_K

EMBEDDING_MODEL = "toshk0/nomic-embed-text-v2-moe:Q6_K" #bge-m3, snowflake-arctic-embed2
LLM_MODEL = "gemma3:4b"
CHUNK_SIZE = 1200 #150,200,300,400,500,600,800,1000,1500,2000
CHUNK_OVERLAP = 120 #50,100,150,200
DB_DIR = "db/db_noq_1200_arctic_chroma"
TOP_K = 12 #2,3,4,5,6,7,8

KB_TEXT = "bengali_kb/doc1_noq.txt"

def get_time_lapsed(start_time, emojis="тП░тП▒я╕П"):
    now_time = time.time()
    time_elapse = now_time-start_time
    print(f"{emojis}   Time elapsed: {time_elapse:.2f} seconds\n")

    return round(time_elapse, 2)


#load the document
loader = TextLoader(KB_TEXT, encoding="utf-8")
docs = loader.load()
print(f"ЁЯУХЁЯУХ Txt Loaded")
print(f"ЁЯФН Preview: {docs[0].page_content[:500]}\n--- PREVIEW END ---\n")

#split the document into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
chunks = splitter.split_documents(docs)

embedding_in_progress = True

def print_elapsed_time():
    start_time = time.time()
    while embedding_in_progress:
        elapsed = int(time.time() - start_time)
        if elapsed % 30 == 0 and elapsed != 0:
            print(f"тП▒я╕П {elapsed} seconds passed...")
        time.sleep(1)

timer_thread = threading.Thread(target=print_elapsed_time)
timer_thread.start()

#embed the chunks and store them in chroma db
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
print(f"ЁЯУжтЬИя╕П Loaded Embedding model {EMBEDDING_MODEL}")
vector_make_start_time = time.time()

if os.path.exists(f"{DB_DIR}/chroma.sqlite3"):
    print(f"ЁЯФСЁЯУж Embedding EXISTS! Loading it...")
    vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
else:
    print(f"ЁЯУжтЭМ Embedding NOT EXIST, creating...")
    print(f"тЬВя╕ПтЬВя╕П Chunk Size: {CHUNK_SIZE}, ЁЯквЁЯкв Overlap: {CHUNK_OVERLAP}")
    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=DB_DIR)

embedding_in_progress = False
timer_thread.join()

print(f"ЁЯФТЁЯФТ Vector store work done...")
get_time_lapsed(vector_make_start_time)
winsound.Beep(1000, 800)

def retrieve(query:str) -> str:
    results = vectorstore.similarity_search(query, k=TOP_K)
    #print(f"ЁЯФСЁЯФР Data retrieved from database")
    with open("current_result.txt", "a", encoding="utf-8") as f:
            f.write("\n\n----------CONTEXT-------------\n\n")

    for i,result in enumerate(results):
        #print(f"ЁЯУЬ result {i+1}: {result.page_content}\n")
        with open("current_result.txt", "a", encoding="utf-8") as f:
            f.write(f"ржкрзНрж░рж╕ржЩрзНржЧ {i+1}:\n")
            f.write(f"{result.page_content}".strip())
            f.write(f"\n\n")

    with open("current_result.txt", "a", encoding="utf-8") as f:
            f.write("\n\n----------CONTEXT END-------------\n\n")

    return "\n\n".join([doc.page_content for doc in results])

def generate_answer(query:str, context: str) -> str:
    #print(f"Generating answer for {query}")
    response = chat(
        model=LLM_MODEL,
        messages=[
            {"role": "user", "content": f"ржкрзНрж░рж╕ржЩрзНржЧ:\n\n{context}\n\nржЙржкрж░рзЗрж░ ржкрзНрж░рж╕ржЩрзНржЧржЯрж┐ ржоржирзЛржпрзЛржЧ ржжрж┐ржпрж╝рзЗ ржкржбрж╝рзЛред \"{query}\" ржкрзНрж░рж╢рзНржиржЯрж┐рж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд, рж╕рж░рж╛рж╕рж░рж┐ ржЙрждрзНрждрж░ ржжрж╛ржУред ржХрзЛржирзЛ ржЕржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ ржмрж╛ ржЕрждрж┐рж░рж┐ржХрзНржд ржХржерж╛ ржжрж┐ржУ ржирж╛ред /no_think"}
        ]
    )
    #print(f"ЁЯУЬЁЯУЬ Response generated")
    return response["message"]["content"]

print(f"тЬИя╕ПтЬИя╕П Using {LLM_MODEL}")

queries = [
    "ржЕржирзБржкржо ржХрж▓рзЗржЬрзЗ ржХрзА рж╕ржорзНржкржирзНржи ржХрж░рзЗржЫрзЗ?",
    "ржмрж┐ржпрж╝рзЗрж░ рж╕ржоржпрж╝ ржХрж▓рзНржпрж╛ржгрзАрж░ ржкрзНрж░ржХрзГржд ржмржпрж╝рж╕ ржХржд ржЫрж┐рж▓?",
    "ржЕржирзБржкржорзЗрж░ рж▓рж╛рж▓ржи-ржкрж╛рж▓ржи ржХрзЗ ржХрж░рзЗржЫрзЗржи?", 
    "ржЕржирзБржкржорзЗрж░ ржорж╛ ржХрзЛржи ржШрж░рзЗрж░ ржорзЗржпрж╝рзЗ?",
    "ржЕржирзБржкржо ржоржирзЗ ржХрж░рзЗ рждрж╛рж░ ржкрзВрж░рзНржг ржмржпрж╝рж╕ рж╣ржпрж╝ржирж┐ ржХрзЗржи?",
    "ржХрж╛ржХрзЗ ржЕржирзБржкржорзЗрж░ ржнрж╛ржЧрзНржп ржжрзЗржмрждрж╛ ржмрж▓рзЗ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ?",
    "ржЕржирзБржкржорзЗрж░ ржнрж╛рж╖рж╛ржпрж╝ рж╕рзБржкрзБрж░рзБрж╖ ржХрж╛ржХрзЗ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ?",
    "ржХрзЛржирзНржиржЧрж░ ржХрж┐",
    "ржЕржирзБржкржорзЗрж░ рж▓ржЬрзНржЬрж╛рж░ ржХрж╛рж░ржг ржХрзА ржЫрж┐рж▓ ржЫрзЗрж▓рзЗржмрзЗрж▓рж╛ржпрж╝?",
    "ржЕржирзБржкржорзЗрж░ ржкрж┐рждрж╛ ржХрзА ржкрзЗрж╢рж╛ржпрж╝ ржЫрж┐рж▓рзЗржи?",
]

query_start_time = time.time()

with open("current_result.txt", "w", encoding="utf-8") as f:
    f.write(f"{LLM_MODEL}, {EMBEDDING_MODEL} тЬВя╕П {CHUNK_SIZE} ЁЯкв {CHUNK_OVERLAP} TOP_K {TOP_K}\n\n")

for index, query in enumerate(queries):
    start_i_now = time.time()

    #print(f"\n{index + 1}/{len(queries)}) {query}")
    with open("current_result.txt", "a", encoding="utf-8") as f:
        f.write(f"{index + 1}/{len(queries)}) {query}\n")
    
    context = retrieve(query)
    answer = generate_answer(query, context)
    #print(f" - {answer}\n")

    time_passed = get_time_lapsed(start_i_now)

    # Append result to the file
    with open("current_result.txt", "a", encoding="utf-8") as f:
        f.write(f" - {answer}\nтП░ЁЯОМ {time_passed} seconds\n\n")

    
    winsound.Beep(1000, 600)

get_time_lapsed(query_start_time, "тП░ЁЯОМЁЯУЬ")
winsound.PlaySound("success.wav", winsound.SND_FILENAME)