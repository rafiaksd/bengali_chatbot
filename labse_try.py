from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from ollama import chat
import os, time, threading, winsound

# Timing helper
def get_time_lapsed(start_time, emojis="тП░тП▒я╕П"):
    now_time = time.time()
    print(f"{emojis}   Time elapsed: {now_time-start_time:.2f} seconds\n")

# Models
EMBEDDING_MODEL_NAME = "sentence-transformers/LaBSE"
LLM_MODEL = "gemma3:12b"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 150
DB_DIR = "db_labse_own_story_word"
TOP_K = 5

# Step 1: Load the text document
loader = TextLoader("bengali_kb/doc1_noq_word.txt", encoding="utf-8")
docs = loader.load()
print(f"ЁЯУХЁЯУХ Txt Loaded")
print(f"ЁЯФН Preview: {docs[0].page_content[:500]}\n--- PREVIEW END ---\n")

# Step 2: Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
chunks = splitter.split_documents(docs)

# Step 3: Load LaBSE embedding model
embedding_in_progress = True

def print_elapsed_time():
    start_time = time.time()
    while embedding_in_progress:
        elapsed = int(time.time() - start_time)
        if elapsed % 30 == 0 and elapsed != 0:
            print(f"тП▒я╕П {elapsed} seconds passed...")
        time.sleep(1)

timer_thread = threading.Thread(target=print_elapsed_time)
timer_thread.start()

# Use HuggingFaceEmbeddings with LaBSE
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
print(f"ЁЯУжтЬИя╕П Loaded Embedding model {EMBEDDING_MODEL_NAME}")
vector_make_start_time = time.time()

# Step 4: Store in Chroma
if os.path.exists(f"{DB_DIR}/chroma.sqlite3"):
    print(f"ЁЯФСЁЯУж Embedding EXISTS! Loading it...")
    vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
else:
    print(f"ЁЯУжтЭМ Embedding NOT EXIST, creating...")
    print(f"тЬВя╕ПтЬВя╕П Chunk Size: {CHUNK_SIZE}, ЁЯквЁЯкв Overlap: {CHUNK_OVERLAP}")
    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=DB_DIR)

embedding_in_progress = False
timer_thread.join()
print(f"ЁЯФТЁЯФТ Vector store work done...")
get_time_lapsed(vector_make_start_time)
winsound.Beep(1000, 800)

# Step 5: RAG Retrieval
def retrieve(query: str) -> str:
    results = vectorstore.similarity_search(query, k=TOP_K)
    return "\n\n".join([doc.page_content for doc in results])

# Step 6: Generate answer with Ollama
def generate_answer(query: str, context: str) -> str:
    response = chat(
        model=LLM_MODEL,
        messages=[
            {"role": "user", "content": f"ржкрзНрж░рж╕ржЩрзНржЧржЯрж┐ ржкржбрж╝рзЛ ржПржмржВ ржкрзНрж░рж╢рзНржирзЗрж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржУ рж╕рж░рж╛рж╕рж░рж┐ ржЙрждрзНрждрж░ ржжрж╛ржУред\n\nржкрзНрж░рж╕ржЩрзНржЧ:\n{context}"},
            {"role": "user", "content": query}
        ]
    )
    return response["message"]["content"]

# Step 7: Ask questions
print(f"тЬИя╕ПтЬИя╕П Using {LLM_MODEL}")

queries = [
    "ржЕржирзБржкржо ржХрж▓рзЗржЬрзЗ ржХрзА рж╕ржорзНржкржирзНржи ржХрж░рзЗржЫрзЗ?",
    "ржХрзЛржирзНржиржЧрж░ ржХрж┐",
    "ржХрж╛ржХрзЗ ржЕржирзБржкржорзЗрж░ ржнрж╛ржЧрзНржп ржжрзЗржмрждрж╛ ржмрж▓рзЗ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ?",
    "ржмрж┐ржпрж╝рзЗрж░ рж╕ржоржпрж╝ ржХрж▓рзНржпрж╛ржгрзАрж░ ржкрзНрж░ржХрзГржд ржмржпрж╝рж╕ ржХржд ржЫрж┐рж▓?",
    "ржЕржирзБржкржорзЗрж░ рж▓рж╛рж▓ржи-ржкрж╛рж▓ржи ржХрзЗ ржХрж░рзЗржЫрзЗржи?", 
    "ржЕржирзБржкржорзЗрж░ ржорж╛ ржХрзЛржи ржШрж░рзЗрж░ ржорзЗржпрж╝рзЗ?",
    "ржЕржирзБржкржо ржоржирзЗ ржХрж░рзЗ рждрж╛рж░ ржкрзВрж░рзНржг ржмржпрж╝рж╕ рж╣ржпрж╝ржирж┐ ржХрзЗржи?",
    "ржЕржирзБржкржорзЗрж░ ржнрж╛рж╖рж╛ржпрж╝ рж╕рзБржкрзБрж░рзБрж╖ ржХрж╛ржХрзЗ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ?",
    "ржЕржирзБржкржорзЗрж░ рж▓ржЬрзНржЬрж╛рж░ ржХрж╛рж░ржг ржХрзА ржЫрж┐рж▓ ржЫрзЗрж▓рзЗржмрзЗрж▓рж╛ржпрж╝?",
    "ржЕржирзБржкржорзЗрж░ ржкрж┐рждрж╛ ржХрзА ржкрзЗрж╢рж╛ржпрж╝ ржЫрж┐рж▓рзЗржи?",
]

query_start_time = time.time()

for index, query in enumerate(queries):
    print(f"\n{index + 1}/{len(queries)}) {query}")
    context = retrieve(query)
    answer = generate_answer(query, context)
    print(f" - {answer}\n")

get_time_lapsed(query_start_time, "тП░ЁЯОМЁЯУЬ")
winsound.PlaySound("success.wav", winsound.SND_FILENAME)
